# AIoT 프로젝트 진행 사항

## 프로젝트 1주차(240708~240714)

### 프로젝트 주제 결정(240708~240910)
* 시각장애인을 위한 경로 안내 서비스
* Jetson Nano와 On-Device AI를 활용하여 필요한 정보를 처리하고 제공
    * 메인 기능 : CV를 이용하여 객체 인식 및 분류 후 후처리를 통해 정보 제공
    * 버스 객체 인식 후 버스 번호 제공(현재 도착한 버스의 번호가 해당 버스인지 파악)
    * 신호등 인식 후 청신호 변경 시 알림 기능 제공(횡단 보도에서 시각장애인용 알림벨이 없는 경우를 대비)
    * 도보 위 장애물 중 킥보드 탐지 후 알림 기능 제공(시각장애인의 지팡이는 낮은 사물에 대해 탐지가 어려워 킥보드에 자주 걸려 넘어지기 때문) 
* 처리한 데이터를 어플리케이션에 제공하여 보호자 및 시각장애인에게 각각 다른 정보 형태로 제공
    * 보호자 : 시각장애인의 현재 위치 정보 제공 및 위험 알림
    * 시각장애인 : 원하는 목적지까지 가능 경로 안내 및 

* 담당 파트 : AI & 하드웨어

### 메인 기능 구현 방법 구상 : Yolo모델을 사용하여 객체인식 모델을 탐지(240710)
* Yolo를 사용하는 이유
    * 합성곱 신경망(CNN) 모델로 컴퓨터 비전에서 뛰어난 성능을 가짐
    * 이미 만들어진 모델로 객체를 학습하는 방법이 쉬움
    * 전이학습(이미 만들어진 모델을 학습)하는 경우, 신경망의 가중치를 이용하기 때문에 높은 정확도를 비교적 짧은 시간 내에 달성할 수 있음
* 버스 번호 확인 기능 : 번호 영역 이미지 crop 후 OCR을 하는 방법으로 구현
    * 번호 영역 이미지 crop 방법
        1. 버스 전면에 있는 번호 영역을 탐지
        2. 버스 전면 탐지 후 번호 영역의 위치를 좌표 계산으로 crop
        3. 차량 번호판을 탐지하고 버스 API와 비교하여 번호 제공
    * OCR 모델
        * EasyOCR : 모델이 무거워 연산 속도가 느리지만 정확한 결과를 제공
        * Tesseract : 모델의 연산이 빠르지만 부정확한 결과가 많이 나올 수 있음

    * 실제 고안 방법 :
        * 빨간 영역 : yolo에서 제공하는 coco128 데이터 셋으로 탐지되는 영역
        * 노란 영역 : 버스 전면부를 인식하는 방법
        * 파란 영역 : 버스 전면부 중 번호 영역을 인식하는 방법

    ![버스 방법 고안](<버스 번호 인식 고안.jpg>)

* 신호등 색 구분 기능
    * 신호등을 빨간색일 경우, 초록색일 경우 학습하여 인식
    * 신호등 영역을 객체 탐지 후, 색 영역과 비교 파악 후 결정

* 킥보드 탐지 알림 기능 : 모델 학습 후, 시야 범위에 객체 탐지 시 알림 기능
    
### Computer Vision 학습(240711)
* 신경망이란?
    * 1개의 입력 계층, 1개이상의 은닉 계층, 1개의 출력 계층으로 구성
    * 각 계층에는 여러 노드 또는 뉴런이 있으며, 각 계층의 노드는 이전 계층의 모든 노드에서 전달되는 출력을 입력으로 사용하여 서로 다른 계층을 연결하고 학습하는 과정에서 가중치가 조정되면서 신호 강도가 변경
    * 신경망은 분류 및 회귀 작업에 사용 가능
* CNN 모델(합성곱 신경망 모델)
    * 구성 : Convolutional Base & Classifier
        * Convolitional Base : 합성곱 층과 풀링층이 여러겹 쌓여있는 부분
            * 이미지로부터 특징을 효과적으로 추출하는 것을 목적으로 만들어짐
        * Classifier : 완전 연결 계층(모든 계층의 뉴런이 이전층의 출력 노드와 하나도 빠짐없이 모두 연결된 층)으로 구성된 부분
    * 낮은 계층에서 입력 받은 이미지의 일반적인 특징을 추출하고 높은 계층에서 구체적이고 특유한 특징을 추출하여 분류
    
    * 학습 방법
        * 1. 전체 모델을 새로 학습
            * 모델을 밑바닥에서부터 새로 학습 시키기 때문에 큰 사이즈의 데이터셋이 필요
        * 2. Convolutional base의 일부분은 고정하고, 나머지 계층과 classifier를 새로 학습
            * 데이터셋이 작고 모델의 파라미터가 많다면 오버피팅이 될 위험이 크므로 더 많은 계층을 건들지 않고 그대로 두어 학습
            * 데이터셋이 크고 그에 비해 모델이 작아서 파라미터가 적다면 오버피팅을 걱정할 필요가 없으므로 더 많은 계층을 학습시켜 적합한 모델로 발전 가능
        * 3. Convolutional base 전체 고정하고, classifier를 학습
            * 특징 추출 메커니즘을 활용하고 classifier만 재학습시키는 방법
            * 컴퓨팅 연산 능력이 부족하거나 데이터 셋이 너무 작을때나 사전 학습모델이 이미 학습한 데이터셋과 매우 비슷할 때 고려하는 방법
        * 1,2번 방법은 learning rate에 대해 조심할 필요가 있음
            * learning rate는 신경망에서 파라미터를 얼마나 재조정할 것인지를 결정하는 하이퍼 파라미터
            * learning rate를 작게 해야 이전의 학습 내용을 잊지 않고 추가학습이 가능

### OpenCV 사용법 학습 및 실습(240711~240712)
* OCR 방법 구현
1. 이미지를 이용하여 필요한 영역을 찾고 해당 영역을 OCR
    * 필요영역을 찾는 방법 : Gray Scale 변경 및 Detour를 활용 코드 구현
        * image CV_tesseract.ipynb 코드 작성
2. OCR에서 EasyOCR과 Pytesseract를 비교

3. 이미지가 아닌 영상을 이용하여 필요한 영역을 탐색하는 코드를 적용하여 OCR을 적용



## 프로젝트 2주차(240715~240721)
### 버스 데이터셋 구축(240715~16)
* 다양한 버스 영상을 준비
* Roboflow를 통한 번호판 영역 이미지 라벨링
* 이미지 라벨링 결과(bus_num, bus_front class - 1549개)
* 버스 전면 번호 영역 모델 학습

### 버스 번호 인식 기능 구현(240717)
* 버스 모델 학습을 이용하여 영상 OCR 코드 작성
    * 버스 번호영역 탐지 시 image Crop을 통해 OCR 모듈로 전달(영역을 조금 더 크게 설정하여 버스 번호 영역 전체가 전달될 수 있도록 코드 작성)
    ```py
    x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
    # 클래스 이름과 신뢰도
    if row['name']=='busnum' and row['confidence']>=0.8:
        bus_roi = frame[int(y1)-5:int(y2)+5, int(x1)-5:int(x2)+5]
        ocr_result = img_ocr(bus_roi)
        if ocr_result:
            label = f'{row['name']} {row['confidence']:.2f}: {ocr_result}' 
            print(label)
    ```
    * 버스 번호 영역 전체를 pytesseract를 이용하여 OCR을 진행

### 학습 모델 Jetson Nano에 deploy 및 실행(240718)
* Jetson Nano에서 3217waiting.MOV 파일 Detecting한 경우 정확한 영역을 인지하여 OCR 결과가 나옴
* 해당 코드를 기반으로 다른 상태에도 동일한 코드를 사용할 수 있도록 구상


## 프로젝트 3주차(240722~240728)
* 신호등 데이터 셋 구축 및 학습(신호등의 색에 따라 Green, Red Class로 분리하여 라벨링)
    * 이미지 라벨링 결과(red, green class - 2032장)
* 킥보드 데이터 셋 구축 및 학습(킥보드의 전 방향을 따라 라벨링)
    * 이미지 라벨링 결과(kickboard class - 1490장)
* 버스 영상 OCR 코드를 Yolo에서 제공하는 Detect.py 를 이용한 코드 작성으로 변경
    * Detect.py에서 보다 정밀한 탐색이 가능하기 때문
    * 버스영상 OCR에서는 문제가 없었으나 신호등의 경우 Detect.py의 코드를 사용하지 않는 경우 객체 인식률이 매우 저조하여 Detect.py를 사용
    * 코드의 동일성을 위해 버스 모델, 킥보드 모델에도 동일하게 Detect.py를 이용한 코드를 사용하도록 변경
    * detect.py 실행시 shell에 작성하는 parse data를 함수로 인자 전달(--view, --source, --data)

* 신호등 모델을 이용한 코드 작성
    * 초록색임을 인지하는 프레임 수를 세고 특정 횟수를 넘기면 초록불 신호를 출력
    * 이 때, 중간에 빨간색으로 인지되면 프레임 계산을 초기화하고 0부터 다시 계산 - 빨간 불일 때 초록불으로 잘못 인지되었을 경우를 방지
    * 신호등을 인지를 잘하지만 초록불과 빨간불 구별이 저조하여 confidence 값을 조정하여 코드작성

* 킥보드 모델을 이용한 코드 작성
    * 킥보드가 사용자의 정면 하단부에 위치할 경우가 시각장애인용 지팡이를 통해 검출이 되지 않고 부딪힐 가능성이 높다고 판단
    * 하단부의 영역을 사각형으로 그리고 해당 영역과 객체를 탐지한 영역이 겹치면 알리는 코드 작성
    * Shapley library 사용(두 도형이 겹치면 True를 반환하는 disjoint를 사용)

* 학습 모델 Jetson Nano에 deploy 및 실행
    * 학습되지 않는 영상 외 새로운 영상을 준비하여 탐지 시 정확한 탐지가 가능하였고 원하는 결과를 도출하게 됨


## 프로젝트 4주차(240729~240804)
### Jetson Nano 구동 코드 작성(240729)
* Jetson에서 실시간 구동을 위한 통신 코드와 While 문을 이용하여 flag 별로 각 모델을 실행할 수 있는 run.py 작성
* run.py에서 Flag 조회 방법 구상
    * IoT 전용 DB를 작성(id, flag, is_completed, result)
    * DB의 id,flag,is_completed를 조회하여 아직 결과가 없는 최신데이터를 조회하고 각 모듈을 실행 후 결과를 해당 id의 result에 저장

* Emergency 상황(사용자의 넘어짐)을 고려한 전역 변수 constant.py 설정
    * gyro를 통해 넘어짐을 검출하면 constant.py에 emergency =1을 덮어씌움, 상황 해제 시 emergency=0을 다시 덮어씌움
    * 모든 상황에서 constant.py를 조회하고 emergency=1일 경우, 모듈 강제 종료 구현

* Gyro를 통해 넘어짐 감지는 Multi Thread를 이용하여 멈춤없이 감지


### 실시간 시연을 위해 영상을 탐지하는 방법에서 실시간 영상을 탐지하는 방법으로 변경(240730~240802)
* 방법을 변경함에 따라 에러 발생 및 해결(240730~240801)
    * detect.py에서 제공하는 실시간 영상을 MSMF와 같은 백엔드 서버에 요청을 보내서 카메라의 영상을 재생하는 방식
    * 해당 방식을 진행하게 되는 경우, 원하는 결과를 얻어서 모듈을 강제 종료시 카메라는 종료되지 않고 계속 재생되어 재실행이 불가능
    * 오류를 해결하기 위해 Detect.py에서 LiveStream을 할 경우 실행되는 LoadStream class 수정
        * class 내부에 cv2 windows 종료하고 camera 객체 연결을 종료하는 cap.release를 넣은 release() method를 추가
        * util/general.py의 imshow를 주석 처리하여 추가로 검사하는 과정에서 카메라 모듈을 키는 것을 방지
        * view image 코드를 False로 설정하여 window에 표시되지 않도록 수정하여 오류 방지
* 방법 변경에 의해 객체 탐지율 하락으로 모델의 동작 변경을 고려

## 프로젝트 5주차(240805~240811)
* 신호등 모델 개선
    * 신호등 색구분 방법 변경 : RED, GREEN Class 분리하여 탐지 -> Traffic 하나의 class를 탐지 후 객체의 사각형 영역을 HSV로 변경하여 초록색과 빨간색의 비율을 비교
    * 신호등 객체 탐지를 늘리기 위해 Dataset 추가 및 Augmentation(Rotate 7, Flip Horizontal)추가 - Dataset 8955장 학습 진행
    * 소리 알림 기능 추가

* 킥보드 모델 개선
    * 킥보드는 기존에 학습되어 있지 않던 케이스였기 때문에 탐지를 제대로 못하였고, 기존 데이터 셋에서 중심에 위치한 경우도 있었지만 프레임 기준 외곽에 걸쳐 있는 데이터가 많이 있어서 실제로 탐지시 외곽에 있는 물체를 탐지할 경우 킥보드로 인식하는 경우가 많음
    * 킥보드 모델 Dataset 추가및 Augmentation추가(Rotate 7, Flip Horizontal)추가 후 학습 진행 - Dataset 13669개 학습 진행
    * 소리 알림 기능 추가

* 라이브 시연 준비
    * 파이브가이즈 강남점의 경로를 촬영하여 영상을 이용
    * 경로에 해당하는 버스 정류장, 신호등, 도보이동이 변경될 때마다 gps 결과로 IOT가 조회하는 DB에 상태를 전송
    * DB에서 지속적인 데이터 조회를 통해 Jetson에서 상태 정보를 저장


## 프로젝트 6주차(240812~240816)
* 버스 모델 개선
    * 라이브 시연을 위해 OCR 성능 향상 :  pytesseract모델에서 Google Vision으로 변경
* MQTT 기능 추가
     * 추가하는 사유 : DB를 조회하는 것은 문제가 되지 않으나, 서버에 있는 DB를 IoT의 데이터 변경시 직접 수정하게 되는 것을 방지
     * MQTT는 publish, subscribe 형식을 이용하기 때문에 가볍고 빠른 데이터 송수신이 가능하고 DB를 직접 수정하는 것이 아니기 때문에 보안에 유리
     * AWS IoT core를 활용하여 서버와 IoT간 송수신을 구현

